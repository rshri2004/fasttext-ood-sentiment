{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa_0FfeMUAol"
      },
      "source": [
        "# 1. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPxrTPLL-Pqa"
      },
      "source": [
        "---------------------\n",
        "1) Loads IMDB and Amazon (polarity) datasets via Hugging Face `datasets`.\n",
        "2) Cleans text (lowercase, remove punctuation, normalize spaces) to match FastText requirements.\n",
        "3) Converts items into FastText supervised format: `__label__positive <text>` or `__label__negative <text>`.\n",
        "4) Samples:\n",
        "- IMDB: 5500 total (50/50 pos/neg) for training source.\n",
        "- Amazon: 500 total (50/50) for OOD testing only.\n",
        "5) Accepts a synthetic dataset file (generated externally) and optionally:\n",
        "- Deduplicates and removes highly similar items using cosine similarity of sentence embeddings.\n",
        "- Performs submodular selection (facility location) to keep a diverse subset.\n",
        "- Synthetic file format expected: one example per line in FastText format, e.g.\n",
        "`__label__positive this movie was absolutely amazing and touching`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxNVOSGX-vLI"
      },
      "source": [
        "Output directory structure (created if missing):\n",
        "\n",
        "- ./data/\n",
        "imdb_train_source.txt # 5500 balanced cleaned samples (real IMDB pool)\n",
        "- amazon_test_500.txt # OOD test set 500 balanced\n",
        "- imdb_test_500.txt # In-distribution test set 500 balanced\n",
        "- ./data/synthetic/synthetic_raw.txt # provided synthetic file (cleaned/normalized)\n",
        "- synthetic_filtered.txt # after similarity filtering + optional submodular selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W1Lgs2P2kNJ",
        "outputId": "fd0afbdf-679f-4d5c-b028-e731a936564d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/kritanjalijain/amazon-reviews?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.29G/1.29G [00:16<00:00, 86.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/kritanjalijain/amazon-reviews/versions/2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['amazon_review_polarity_csv.tgz', 'train.csv', 'test.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kritanjalijain/amazon-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "import os\n",
        "os.listdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRLA7K456iDV",
        "outputId": "9d9006c5-efc8-4946-ba1d-290c10c5a627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'imdb-dataset-of-50k-movie-reviews' dataset.\n",
            "Path to dataset files: /kaggle/input/imdb-dataset-of-50k-movie-reviews\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['IMDB Dataset.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "import os\n",
        "os.listdir(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wygJTnM57zTt"
      },
      "outputs": [],
      "source": [
        "# ==== Data preprocessing only: IMDB (Kaggle CSV) + Amazon Review Polarity (Zhang) ====\n",
        "import os, re, tarfile, random\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "\n",
        "# ---- Input paths (edit if needed) ----\n",
        "IMDB_CSV = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n",
        "AMZ_TGZ  = \"/kaggle/input/amazon-reviews/amazon_review_polarity_csv.tgz\"  # optional; will be extracted if present\n",
        "AMZ_TRAIN = \"/kaggle/input/amazon-reviews/train.csv\"\n",
        "AMZ_TEST  = \"/kaggle/input/amazon-reviews/test.csv\"\n",
        "\n",
        "#\n",
        "#IMDB_CSV = \"/root/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv\"\n",
        "# AMZ_TGZ = \"/root/.cache/kagglehub/datasets/kritanjalijain/amazon-reviews/versions/2/amazon_review_polarity_csv.tgz\"\n",
        "# AMZ_TRAIN = \"/root/.cache/kagglehub/datasets/kritanjalijain/amazon-reviews/versions/2/train.csv\"\n",
        "# AMZ_TEST = \"/root/.cache/kagglehub/datasets/kritanjalijain/amazon-reviews/versions/2/test.csv\"\n",
        "\n",
        "# ---- Output dirs ----\n",
        "os.makedirs(\"data/final_splits\", exist_ok=True)\n",
        "os.makedirs(\"data/vocabs\", exist_ok=True)\n",
        "\n",
        "# ---- Basic cleaner (FastText-style) ----\n",
        "PUNCT_RE = re.compile(r\"[^a-z0-9\\s]\")\n",
        "SPACE_RE = re.compile(r\"\\s+\")\n",
        "FASTTEXT_POS = \"__label__positive\"\n",
        "FASTTEXT_NEG = \"__label__negative\"\n",
        "\n",
        "def base_clean_text(s: str) -> str:\n",
        "    s = str(s).lower()\n",
        "    s = PUNCT_RE.sub(\" \", s)\n",
        "    s = SPACE_RE.sub(\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "def to_fasttext(label: str, text: str) -> str:\n",
        "    lab = FASTTEXT_POS if label == \"positive\" else FASTTEXT_NEG\n",
        "    return f\"{lab} {text}\"\n",
        "\n",
        "def save_lines(path: str, lines: List[str]):\n",
        "    d = os.path.dirname(path)\n",
        "    if d: os.makedirs(d, exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for ln in lines:\n",
        "            f.write(ln + \"\\n\")\n",
        "\n",
        "def save_vocab(path: str, lines: List[str]):\n",
        "    cnt = Counter()\n",
        "    for l in lines:\n",
        "        parts = l.split(\" \", 1)\n",
        "        if len(parts) == 2:\n",
        "            cnt.update(parts[1].split())\n",
        "    d = os.path.dirname(path)\n",
        "    if d: os.makedirs(d, exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for tok, c in cnt.most_common():\n",
        "            f.write(f\"{tok}\\t{c}\\n\")\n",
        "\n",
        "# ---- 0) Extract Amazon polarity if we only have the .tgz ----\n",
        "if os.path.exists(AMZ_TGZ):\n",
        "    try:\n",
        "        with tarfile.open(AMZ_TGZ, \"r:gz\") as tar:\n",
        "          tar.extractall(path=\".\", filter=\"data\") # Safer default: only extract regular files\n",
        "        print(\"Extracted amazon_review_polarity_csv.tgz\")\n",
        "    except Exception as e:\n",
        "        print(\"Warning: could not extract .tgz:\", e)\n",
        "\n",
        "# ---- 1) IMDB (Kaggle) -> pool + 500 test ----\n",
        "if not os.path.exists(IMDB_CSV):\n",
        "    raise FileNotFoundError(f\"Missing {IMDB_CSV}. Update IMDB_CSV path at the top of the cell.\")\n",
        "\n",
        "imdb = pd.read_csv(IMDB_CSV)  # columns: review, sentiment (Kaggle)\n",
        "text_col = \"review\" if \"review\" in imdb.columns else imdb.columns[0]\n",
        "label_col = \"sentiment\" if \"sentiment\" in imdb.columns else imdb.columns[1]\n",
        "imdb = imdb[[text_col, label_col]].dropna()\n",
        "\n",
        "# Clean + map labels\n",
        "imdb[text_col] = imdb[text_col].astype(str).apply(base_clean_text)\n",
        "imdb[label_col] = (\n",
        "    imdb[label_col].astype(str).str.lower().map({\n",
        "        \"positive\": \"positive\", \"pos\": \"positive\", \"1\": \"positive\",\n",
        "        \"negative\": \"negative\", \"neg\": \"negative\", \"0\": \"negative\"\n",
        "    })\n",
        ")\n",
        "imdb = imdb[imdb[label_col].isin([\"positive\", \"negative\"])]\n",
        "\n",
        "# Split\n",
        "imdb_pos = imdb[imdb[label_col] == \"positive\"][text_col].tolist()\n",
        "imdb_neg = imdb[imdb[label_col] == \"negative\"][text_col].tolist()\n",
        "random.shuffle(imdb_pos); random.shuffle(imdb_neg)\n",
        "\n",
        "# Pool ~5.5k (2750/2750); you can later sample 5k for training if needed\n",
        "pos_pool = imdb_pos[:2750]\n",
        "neg_pool = imdb_neg[:2750]\n",
        "imdb_pool_lines = [to_fasttext(\"positive\", t) for t in pos_pool] + \\\n",
        "                  [to_fasttext(\"negative\", t) for t in neg_pool]\n",
        "random.shuffle(imdb_pool_lines)\n",
        "\n",
        "# IMDB test 500 (250/250), prefer using rows beyond pool if available\n",
        "pos_test = imdb_pos[2750:3000] if len(imdb_pos) >= 3000 else imdb_pos[:250]\n",
        "neg_test = imdb_neg[2750:3000] if len(imdb_neg) >= 3000 else imdb_neg[:250]\n",
        "pos_test = pos_test[:250]\n",
        "neg_test = neg_test[:250]\n",
        "imdb_test_lines = [to_fasttext(\"positive\", t) for t in pos_test] + \\\n",
        "                  [to_fasttext(\"negative\", t) for t in neg_test]\n",
        "random.shuffle(imdb_test_lines)\n",
        "\n",
        "# ---- 2) Amazon Review Polarity (Zhang) -> 500 test (250/250) ----\n",
        "# CSV format is usually: [label, title, text] or [label, text]; labels: 1=negative, 2=positive\n",
        "def _read_amazon_csv(csv_path: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path, header=None)\n",
        "    except Exception:\n",
        "        df = pd.read_csv(csv_path, header=None, engine=\"python\", sep=\",\", quoting=3, on_bad_lines=\"skip\")\n",
        "    if df.shape[1] == 3:\n",
        "        df.columns = [\"label\", \"title\", \"text\"]\n",
        "        df[\"text\"] = df[\"text\"].astype(str)\n",
        "    elif df.shape[1] == 2:\n",
        "        df.columns = [\"label\", \"text\"]\n",
        "        df[\"text\"] = df[\"text\"].astype(str)\n",
        "    else:\n",
        "        df[\"text\"] = df.iloc[:, -1].astype(str)\n",
        "        df[\"label\"] = df.iloc[:, 0]\n",
        "    return df[[\"label\", \"text\"]]\n",
        "\n",
        "\n",
        "if os.path.exists(AMZ_TEST):\n",
        "    amz_df = _read_amazon_csv(AMZ_TEST)\n",
        "elif os.path.exists(AMZ_TRAIN):\n",
        "    amz_df = _read_amazon_csv(AMZ_TRAIN)\n",
        "else:\n",
        "    raise FileNotFoundError(\"Amazon polarity CSVs not found (train.csv/test.csv).\")\n",
        "\n",
        "def _map_amz_label(v):\n",
        "    try:\n",
        "        v = int(v)\n",
        "    except Exception:\n",
        "        return None\n",
        "    if v == 1: return \"negative\"\n",
        "    if v == 2: return \"positive\"\n",
        "    return None\n",
        "\n",
        "amz_df = amz_df.dropna()\n",
        "amz_df[\"label_bin\"] = amz_df[\"label\"].apply(_map_amz_label)\n",
        "amz_df = amz_df[amz_df[\"label_bin\"].notna()].copy()\n",
        "amz_df[\"text\"] = amz_df[\"text\"].astype(str).apply(base_clean_text)\n",
        "\n",
        "amz_pos = amz_df[amz_df[\"label_bin\"] == \"positive\"][\"text\"].tolist()\n",
        "amz_neg = amz_df[amz_df[\"label_bin\"] == \"negative\"][\"text\"].tolist()\n",
        "random.shuffle(amz_pos); random.shuffle(amz_neg)\n",
        "\n",
        "amz_pos = amz_pos[:250] if len(amz_pos) >= 250 else amz_pos\n",
        "amz_neg = amz_neg[:250] if len(amz_neg) >= 250 else amz_neg\n",
        "amazon_test_lines = [to_fasttext(\"positive\", t) for t in amz_pos] + \\\n",
        "                    [to_fasttext(\"negative\", t) for t in amz_neg]\n",
        "random.shuffle(amazon_test_lines)\n",
        "\n",
        "# ---- 3) Save outputs ----\n",
        "save_lines(\"data/imdb_train_source.txt\", imdb_pool_lines)\n",
        "save_lines(\"data/imdb_test_500.txt\", imdb_test_lines)\n",
        "save_lines(\"data/amazon_test_500.txt\", amazon_test_lines)\n",
        "\n",
        "# ---- 3.1) Optional: also save an exact 5,000-line balanced IMDB training set ----\n",
        "random.seed(SEED)\n",
        "\n",
        "# separate positive and negative lines from the 5.5k pool\n",
        "pos = [l for l in imdb_pool_lines if l.startswith(FASTTEXT_POS)]\n",
        "neg = [l for l in imdb_pool_lines if l.startswith(FASTTEXT_NEG)]\n",
        "\n",
        "# take 2,500 of each → total 5,000\n",
        "k = min(2500, len(pos), len(neg))\n",
        "real_5k = pos[:k] + neg[:k]\n",
        "random.shuffle(real_5k)\n",
        "\n",
        "save_lines(\"data/final_splits/real_5k.txt\", real_5k)\n",
        "save_vocab(\"data/vocabs/vocab_real_5k.txt\", real_5k)\n",
        "\n",
        "print(f\"Saved balanced IMDB training set: data/final_splits/real_5k.txt ({len(real_5k)} lines)\")\n",
        "\n",
        "\n",
        "\n",
        "# (Optional) quick vocabs for sanity check\n",
        "save_vocab(\"data/vocabs/vocab_imdb_pool.txt\", imdb_pool_lines)\n",
        "save_vocab(\"data/vocabs/vocab_imdb_test_500.txt\", imdb_test_lines)\n",
        "save_vocab(\"data/vocabs/vocab_amazon_test_500.txt\", amazon_test_lines)\n",
        "\n",
        "# ---- 4) Report ----\n",
        "print(\"  Preprocessing complete.\")\n",
        "print(\"  IMDB pool:\", len(imdb_pool_lines), \"(~5500 target)\")\n",
        "print(\"  IMDB test:\", len(imdb_test_lines), \"(target 500)\")\n",
        "print(\"  Amazon test:\", len(amazon_test_lines), \"(target 500)\")\n",
        "print(\"\\nFiles written:\")\n",
        "for p in [\n",
        "    \"data/imdb_train_source.txt\",\n",
        "    \"data/imdb_test_500.txt\",\n",
        "    \"data/amazon_test_500.txt\",\n",
        "    \"data/vocabs/vocab_imdb_pool.txt\",\n",
        "    \"data/vocabs/vocab_imdb_test_500.txt\",\n",
        "    \"data/vocabs/vocab_amazon_test_500.txt\",\n",
        "]:\n",
        "    print(\" -\", p, \"→\", os.path.exists(p))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs3-l2qv9db2",
        "outputId": "201df687-85be-43d1-8305-424ee5553648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   5500 data/imdb_train_source.txt\n",
            "    500 data/imdb_test_500.txt\n",
            "    500 data/amazon_test_500.txt\n",
            "   6500 total\n",
            "__label__positive lead actor yuko tanaka fulfills so much in the exceptionally meditative the milkwoman a tranquil canvass on missed chances in the life of a 50 something woman charting her routine with sincerely poignant motives played out in the picturesque tranquil town of nagasaki akira ogata s unconventional romantic film so to speak is less a straight out melodrama than a deliberate introspection of its characters surrender to their current lives as a result of a tragic past that forced them to a choice they did not call for br br perfectly embodying the requisite world weariness subjected to a spiritless routine tanaka plays minako oba a middle aged woman who before her work shift at a supermarket takes it upon herself to deliver bottles of milk among the residents of the hilly nagasaki one of the houses she constantly passes by to make such a delivery is that of kaita takanashi ittoku kishibe a local government employee caring for her terminally ill wife akiko nishina minako and kaita were high school sweethearts who courtesy of an ignominious event concerning their parents separated ways since then br br opening his film with the foreboding narration of a young minako vowing never to leave nagasaki ogata does as such with the narrative patiently sticking with minako as he deftly aided by tanako s understated yet highly effective performance follows her whether she s having chitchat with her aunt misako watanabe on being single or when she jogs up and down the countless footsteps of their hilly town to distribute milk as she and kaita gradually overcome the hindrances that kept them apart for years such unhurried development may not suit viewers weaned on fast paced narratives but for the rest it s a heartfelt introspection that affects powerfully and emphatically\n",
            "__label__negative this movie has some of the most awesome cars i ve ever seen in a movie and definitely the hottest women but i would have to say it is still one of the worst movies i ve ever seen br br here is the plot and if you read it with a little inflection you have the acting as well br br beginning bring in characters hot woman singing obvious lip sync music agent or producer comes in thinks that she is awesome asks her to race she turns down too many bad memories flash to war hero back from war has several fights and becomes movie hero with attitude that he is better than everyone drive off in fast exotic car brother races then dies hero to avenge death cut away to getting weapons from friend you have never seen this friend before or after but seems to really care about him are you sure you want to do this yes i mean are you really sure yes give me weapons are you really sure yes ok i guess i can t talk you out of it be careful man i love you br br now he goes to blow up his uncles house who owned the car his brother drove finds woman decides to rescue her she drives off and he doesn t finish killing his uncle now there will be a race to finish the movie oh yeah need to throw in one more scene with bad people coming in to beat up people that don t really matter but maybe it adds a little plot race is not even that exciting of course it ends with two cars racing and one that should win throws in a surprise ending br br ok i just saved you 7 00 you can send all of your money to me because i should have given you the same amount of enjoyment as this movie does don t get me wrong the cars are awesome and nadija is beautiful but it is truly an awful movie\n",
            "__label__negative dark harvest is about a group of friends that go to a farm it belongs to one of the friends relatives or something for a getaway but there are killer scarecrows lurking there there was something about a curse in there too but i forgot what that was about br br the acting in this movie is awful i don t know what the director was thinking when he was casting actors and actresses the script is the same story as the acting awful this statement coming up is very obvious but if there was better acting and a better script this could have turned out okay br br the directing stunk too i see no potential in this guy s future after all these negatives this movie still maintains a fun factor that bumps it up to a two the last plus is they don t use cgi my overall thoughts on this film are it s bad real bad but so bad it s fun so it gets a 2 10\n",
            "__label__negative i ve had my kindle less than a month and the battery won t recharge now i see i can t get another one for a while what is wrong with you people didn t anyone ever tell you the product has to work for more than a few days before the battery fails if you can t get me a replacement battery and the battery has such a crappy life what use is the kindle to me seriously guys\n",
            "__label__negative it says it has 800 blocks of memory but i can only save 1 season of nfl2k1 on it don t buy it i own 2 and both do the same thing\n",
            "__label__negative one star for levi s 550 s five stars for amazon customer service i recently ordered two pairs of 550 s from amazon expecting the same quality they ve always had eh not so much first thing i noticed when i took them out was the thin material they don t feel like jeans more like dockers chinos also i noticed the stitching wasn t particularly tight i returned both pairs for full refund thanks amazon\n"
          ]
        }
      ],
      "source": [
        "\"\"\" This code just for checking and tiny peeks of the reviews \"\"\"\n",
        "\n",
        "# line counts\n",
        "!wc -l data/imdb_train_source.txt data/imdb_test_500.txt data/amazon_test_500.txt\n",
        "\n",
        "# peek a few rows\n",
        "!head -n 3 data/imdb_test_500.txt\n",
        "!head -n 3 data/amazon_test_500.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_oy0XtsUFFx"
      },
      "source": [
        "# 2. Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGoCFELnue9J"
      },
      "source": [
        "synthetic data generated via ChatGPT webapp using the GPT-5 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V2rFT1puzZT"
      },
      "source": [
        "Code to shuffle the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "RqYRJl9LT_58",
        "outputId": "f043d30b-33ca-469e-9c84-8163d0440dbc"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '5050RealSynthetic.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2971543260.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#open the input file and read each line into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdatasetUnshuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdatasetLines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasetUnshuffled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetLines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '5050RealSynthetic.txt'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "np.random.seed(42)\n",
        "\n",
        "#Uncomment the necessary file names to shuffle the 100% synthetic or 50/50 real/synthetic dataset\n",
        "# inputFile = \"syntheticData.txt\"\n",
        "# outputFile = \"shuffledSyntheticData.txt\"\n",
        "inputFile = \"5050RealSynthetic.txt\"\n",
        "outputFile = \"shuffled5050RealSynthetic.txt\"\n",
        "\n",
        "#open the input file and read each line into a list\n",
        "datasetUnshuffled = open(inputFile)\n",
        "datasetLines = datasetUnshuffled.readlines()\n",
        "print(len(datasetLines))\n",
        "\n",
        "#shuffle indices\n",
        "index = np.arange(0, len(datasetLines), 1, dtype= int)\n",
        "print(\"index has length: \", len(index))\n",
        "np.random.shuffle(index)\n",
        "print(\"after shuffling index has length: \", len(index))\n",
        "\n",
        "#if the output file exists, write each line of the input file in the order of the shuffled indices\n",
        "#if the output file doesn't exist, exit\n",
        "if os.path.exists(outputFile):\n",
        "    datasetShuffled = open(outputFile, \"w\")\n",
        "    count = 0\n",
        "    for idx in index:\n",
        "        datasetShuffled.write(datasetLines[idx])\n",
        "        count += 1\n",
        "    print(\"count is: \", count)\n",
        "    datasetUnshuffled.close()\n",
        "    datasetShuffled.close()\n",
        "    datasetShuffled = open(outputFile, \"r\")\n",
        "    print(len(datasetShuffled.readlines()))\n",
        "    print(\"finished shuffling the dataset!\")\n",
        "else:\n",
        "    datasetUnshuffled.close()\n",
        "    print(f\"{outputFile} doesn't exist! Please create the file and try again\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-jUAj1QvTSi"
      },
      "source": [
        "Code to generate the 50/50 real/synthetic data (needs to be shuffled after generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "GPtV4DTivcQJ",
        "outputId": "964fd303-6a0f-4019-9b06-e39dfa6570a5"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'real_5k.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20328803.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#open the real and synthetic datasets and read the lines. Separate the lines into positive and negative reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mrealData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"real_5k.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mrealLines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrealData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mrealPositiveList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'real_5k.txt'"
          ]
        }
      ],
      "source": [
        "limit = 1250\n",
        "realPositiveCount = 0\n",
        "realNegativeCount = 0\n",
        "synthPositiveCount = 0\n",
        "synthNegativeCount = 0\n",
        "\n",
        "#a function to determine the sentiment label of a review\n",
        "def parseLabel(line):\n",
        "    if \"__label__positive\" in line:\n",
        "        return \"positive\"\n",
        "    elif \"__label__negative\" in line:\n",
        "        return \"negative\"\n",
        "\n",
        "#open the real and synthetic datasets and read the lines. Separate the lines into positive and negative reviews\n",
        "np.random.seed(42)\n",
        "realData = open(\"real_5k.txt\")\n",
        "realLines = realData.readlines()\n",
        "realPositiveList = []\n",
        "realNegativeList = []\n",
        "for line in realLines:\n",
        "    if parseLabel(line) == \"positive\":\n",
        "        realPositiveList.append(line)\n",
        "    elif parseLabel(line) == \"negative\":\n",
        "        realNegativeList.append(line)\n",
        "\n",
        "syntheticData = open(\"shuffledSyntheticData.txt\")\n",
        "syntheticLines = syntheticData.readlines()\n",
        "syntheticPositiveList = []\n",
        "syntheticNegativeList = []\n",
        "for line in syntheticLines:\n",
        "    if parseLabel(line) == \"positive\":\n",
        "        syntheticPositiveList.append(line)\n",
        "    elif parseLabel(line) == \"negative\":\n",
        "        syntheticNegativeList.append(line)\n",
        "\n",
        "realData.close()\n",
        "syntheticData.close()\n",
        "\n",
        "print(\"We have: \", len(realPositiveList), \" real positive examples\")\n",
        "print(\"We have: \", len(realNegativeList), \" real negative examples\")\n",
        "print(\"We have: \", len(syntheticPositiveList), \" synthetic positive examples\")\n",
        "print(\"We have: \", len(syntheticNegativeList), \" synthetic negative examples\")\n",
        "\n",
        "\n",
        "index = np.arange(0, 5000, 1, dtype= int)\n",
        "\n",
        "lists = [realPositiveList, realNegativeList, syntheticPositiveList, syntheticNegativeList]\n",
        "listLimit = [realPositiveCount, realNegativeCount, synthPositiveCount, synthNegativeCount]\n",
        "#create the output file and randomly sample a uniform amount of examples from each of the four subcategories\n",
        "with open(\"5050RealSynthetic.txt\", \"w\") as f:\n",
        "    for i in range(lists):\n",
        "        np.random.shuffle(index)\n",
        "        while listLimit[i] < limit:\n",
        "            idx = listLimit[i]\n",
        "            f.write(lists[i][index[idx]])\n",
        "            listLimit[i] += 1\n",
        "\n",
        "\n",
        "print(\"finished sampling\")\n",
        "\n",
        "totalPositiveCount = 0\n",
        "totalNegativeCount = 0\n",
        "totalLineCount = 0\n",
        "with open(\"5050RealSynthetic.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        if parseLabel(line) == \"positive\":\n",
        "            totalPositiveCount += 1\n",
        "        elif parseLabel(line) == \"negative\":\n",
        "            totalNegativeCount += 1\n",
        "        totalLineCount += 1\n",
        "\n",
        "print(\"there are \", totalPositiveCount, \" positive examples\")\n",
        "print(\"there are \", totalNegativeCount, \" negative examples\")\n",
        "print(\"there are \", totalLineCount, \" total examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Jksei_ogsB"
      },
      "source": [
        "Code to generate the appended dataset (needs to be shuffled after generation)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e88YBdUogDd"
      },
      "outputs": [],
      "source": [
        "def parseLabel(line):\n",
        "    if \"__label__positive\" in line:\n",
        "        return \"positive\"\n",
        "    elif \"__label__negative\" in line:\n",
        "        return \"negative\"\n",
        "\n",
        "def appendExample(ex1, ex2):\n",
        "    return ex1[:-1] + \" \" + ex2[18:]\n",
        "\n",
        "np.random.seed(42)\n",
        "realData = open(\"real_5k.txt\")\n",
        "realLines = realData.readlines()\n",
        "realPositiveList = []\n",
        "realNegativeList = []\n",
        "for line in realLines:\n",
        "    if parseLabel(line) == \"positive\":\n",
        "        realPositiveList.append(line)\n",
        "    elif parseLabel(line) == \"negative\":\n",
        "        realNegativeList.append(line)\n",
        "\n",
        "syntheticData = open(\"shuffledSyntheticData.txt\")\n",
        "syntheticLines = syntheticData.readlines()\n",
        "syntheticPositiveList = []\n",
        "syntheticNegativeList = []\n",
        "for line in syntheticLines:\n",
        "    if parseLabel(line) == \"positive\":\n",
        "        syntheticPositiveList.append(line)\n",
        "    elif parseLabel(line) == \"negative\":\n",
        "        syntheticNegativeList.append(line)\n",
        "\n",
        "realData.close()\n",
        "syntheticData.close()\n",
        "\n",
        "print(\"We have: \", len(realPositiveList), \" real positive examples\")\n",
        "print(\"We have: \", len(realNegativeList), \" real negative examples\")\n",
        "print(\"We have: \", len(syntheticPositiveList), \" synthetic positive examples\")\n",
        "print(\"We have: \", len(syntheticNegativeList), \" synthetic negative examples\")\n",
        "\n",
        "if len(realPositiveList) != len(realNegativeList) or len(realPositiveList) != len(syntheticPositiveList) or len(realPositiveList) != len(syntheticPositiveList) or len(realNegativeList) != len(syntheticPositiveList) or len(realNegativeList) != len(syntheticNegativeList) or len(syntheticPositiveList) != len(syntheticNegativeList):\n",
        "    print(\"The list lengths are not all equal.\")\n",
        "    exit()\n",
        "\n",
        "with open(\"appendedRealSynthetic.txt\", \"w\") as f:\n",
        "    for i in range(len(realPositiveList)):\n",
        "        f.write(appendExample(realPositiveList[i], syntheticPositiveList[i]))\n",
        "        f.write(appendExample(realNegativeList[i], syntheticNegativeList[i]))\n",
        "\n",
        "print(\"finished appending\")\n",
        "\n",
        "totalPositiveCount = 0\n",
        "totalNegativeCount = 0\n",
        "totalLineCount = 0\n",
        "with open(\"appendedRealSynthetic.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        # print(line)\n",
        "        if parseLabel(line) == \"positive\":\n",
        "            totalPositiveCount += 1\n",
        "        elif parseLabel(line) == \"negative\":\n",
        "            totalNegativeCount += 1\n",
        "        totalLineCount += 1\n",
        "\n",
        "print(\"there are \", totalPositiveCount, \" positive examples\")\n",
        "print(\"there are \", totalNegativeCount, \" negative examples\")\n",
        "print(\"there are \", totalLineCount, \" total examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzATJ5KYoNr7"
      },
      "source": [
        "# 3. Submodular Data Subset Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9ycyUtgpI75"
      },
      "source": [
        "###Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psaUqzh-pOHW"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn tqdm -q\n",
        "\n",
        "print(\"Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0-Oo__upZYs"
      },
      "source": [
        "###Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd3eAe-dr_m_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from typing import List, Tuple\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Configuration\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Paths\n",
        "REAL_5K = \"/content/data/input/real_5k.txt\"\n",
        "SYNTHETIC_5K = \"/content/data/input/shuffledSyntheticData.txt\"\n",
        "MIXED_5K = \"/content/data/input/5050RealSynthetic.txt\"\n",
        "\n",
        "OUTPUT_DIR = \"data/submodular_selected\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB1jbXt0sSXI"
      },
      "source": [
        "###Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A7GkU9msVWP"
      },
      "outputs": [],
      "source": [
        "def load_fasttext_data(filepath: str) -> List[Tuple[str, str]]:\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith(\"__label__positive\"):\n",
        "                data.append((\"positive\", line.replace(\"__label__positive\", \"\").strip()))\n",
        "            elif line.startswith(\"__label__negative\"):\n",
        "                data.append((\"negative\", line.replace(\"__label__negative\", \"\").strip()))\n",
        "    return data\n",
        "\n",
        "def save_fasttext_data(filepath: str, data: List[Tuple[str, str]]):\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        for label, text in data:\n",
        "            f.write(f\"__label__{label} {text}\\n\")\n",
        "\n",
        "print(\"Functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVhWueQ-tFnY"
      },
      "source": [
        "###Check Input Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiWZEcLotIQj"
      },
      "outputs": [],
      "source": [
        "print(\"Checking for input files...\\n\")\n",
        "\n",
        "for name, path in [(\"Real 5k\", REAL_5K),\n",
        "                   (\"Synthetic 5k\", SYNTHETIC_5K),\n",
        "                   (\"Mixed 5k\", MIXED_5K)]:\n",
        "    exists = os.path.exists(path)\n",
        "    status = \"Found\" if exists else \"Not found\"\n",
        "    print(f\"{name}: {status}\")\n",
        "    if exists:\n",
        "        with open(path) as f:\n",
        "            count = sum(1 for _ in f)\n",
        "        print(f\"   → {count} lines\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6ysDp8K1_jo"
      },
      "source": [
        "###Facility Location Datatset Selector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQyuToCv2L1i"
      },
      "outputs": [],
      "source": [
        "class FacilityLocationSelector:\n",
        "    \"\"\"\n",
        "    Optimized submodular selection with batched processing.\n",
        "\n",
        "    Speed improvements:\n",
        "    1. Precompute similarity matrix (done once)\n",
        "    2. Batch candidate evaluation (check 100 at a time)\n",
        "    3. Early stopping when gain plateaus\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tfidf_matrix, labels: List[str]):\n",
        "        self.tfidf_matrix = tfidf_matrix\n",
        "        self.labels = np.array(labels)\n",
        "        self.n_samples = tfidf_matrix.shape[0]\n",
        "\n",
        "        print(\"Computing similarity matrix (this takes time but speeds up selection)...\")\n",
        "        # Precompute full similarity matrix\n",
        "        self.similarity = cosine_similarity(tfidf_matrix)\n",
        "        print(f\"Similarity matrix: {self.similarity.shape}\")\n",
        "\n",
        "    def select_greedy(self, k: int, stratified: bool = True) -> List[int]:\n",
        "        \"\"\"Fast greedy selection with stratification.\"\"\"\n",
        "        if stratified:\n",
        "            pos_indices = np.where(self.labels == 'positive')[0]\n",
        "            neg_indices = np.where(self.labels == 'negative')[0]\n",
        "\n",
        "            k_pos = k // 2\n",
        "            k_neg = k - k_pos\n",
        "\n",
        "            print(f\"Selecting {k_pos} positive + {k_neg} negative = {k} total\")\n",
        "\n",
        "            selected_pos = self._fast_select(pos_indices, k_pos)\n",
        "            selected_neg = self._fast_select(neg_indices, k_neg)\n",
        "\n",
        "            return list(selected_pos) + list(selected_neg)\n",
        "        else:\n",
        "            return self._fast_select(np.arange(self.n_samples), k)\n",
        "\n",
        "    def _fast_select(self, candidate_indices: np.ndarray, k: int) -> List[int]:\n",
        "        \"\"\"\n",
        "        FAST greedy selection using vectorized operations.\n",
        "\n",
        "        Key optimization: Use precomputed similarity matrix\n",
        "        \"\"\"\n",
        "        selected = []\n",
        "        remaining = set(candidate_indices)\n",
        "\n",
        "        # Coverage: max similarity from each point to selected set\n",
        "        coverage = np.full(self.n_samples, -np.inf)\n",
        "\n",
        "        pbar = tqdm(range(k), desc=\"Fast selection\")\n",
        "\n",
        "        for iteration in pbar:\n",
        "            # Convert remaining to array for vectorized ops\n",
        "            remaining_arr = np.array(list(remaining))\n",
        "\n",
        "            if len(remaining_arr) == 0:\n",
        "                break\n",
        "\n",
        "            # Vectorized gain computation\n",
        "            # For each candidate, compute new coverage if we add it\n",
        "            best_idx = None\n",
        "            best_gain = -np.inf\n",
        "\n",
        "            # Process in batches for memory efficiency\n",
        "            batch_size = 100\n",
        "            for i in range(0, len(remaining_arr), batch_size):\n",
        "                batch = remaining_arr[i:i+batch_size]\n",
        "\n",
        "                # Get similarities for this batch\n",
        "                batch_sims = self.similarity[:, batch]  # (n_samples, batch_size)\n",
        "\n",
        "                # Compute new coverage for each in batch\n",
        "                new_coverage = np.maximum(coverage[:, np.newaxis], batch_sims)  # (n_samples, batch_size)\n",
        "\n",
        "                # Compute gains\n",
        "                gains = np.sum(new_coverage, axis=0) - np.sum(coverage)  # (batch_size,)\n",
        "\n",
        "                # Find best in batch\n",
        "                batch_best_idx = np.argmax(gains)\n",
        "                batch_best_gain = gains[batch_best_idx]\n",
        "\n",
        "                if batch_best_gain > best_gain:\n",
        "                    best_gain = batch_best_gain\n",
        "                    best_idx = batch[batch_best_idx]\n",
        "\n",
        "            # Add best\n",
        "            selected.append(best_idx)\n",
        "            remaining.remove(best_idx)\n",
        "\n",
        "            # Update coverage\n",
        "            coverage = np.maximum(coverage, self.similarity[:, best_idx])\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'gain': f'{best_gain:.1f}',\n",
        "                'remaining': len(remaining)\n",
        "            })\n",
        "\n",
        "        return selected\n",
        "\n",
        "    def compute_diversity_score(self, selected_indices: List[int]) -> float:\n",
        "        \"\"\"Compute facility location objective.\"\"\"\n",
        "        if not selected_indices:\n",
        "            return 0.0\n",
        "\n",
        "        selected_sims = self.similarity[:, selected_indices]\n",
        "        max_sims = np.max(selected_sims, axis=1)\n",
        "        return float(np.sum(max_sims))\n",
        "\n",
        "print(\"FacilityLocationSelector loaded\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkS2pDjz2UCd"
      },
      "source": [
        "###Main Processing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg358DQ62uUY"
      },
      "outputs": [],
      "source": [
        "def process_dataset(input_file: str, output_prefix: str, target_size: int = 3000):\n",
        "    \"\"\"Process dataset with fast submodular selection.\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing: {input_file}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Load\n",
        "    print(\"[1/4] Loading data...\")\n",
        "    data = load_fasttext_data(input_file)\n",
        "    labels = [label for label, text in data]\n",
        "    texts = [text for label, text in data]\n",
        "    print(f\" Loaded {len(data)} examples\")\n",
        "\n",
        "    # TF-IDF\n",
        "    print(\"\\n[2/4] Computing TF-IDF...\")\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=2,\n",
        "        max_df=0.95\n",
        "    )\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    print(f\" TF-IDF: {tfidf_matrix.shape}\")\n",
        "\n",
        "    # Selection\n",
        "    print(\"\\n[3/4] Fast submodular selection...\")\n",
        "    selector = FacilityLocationSelector(tfidf_matrix, labels)\n",
        "\n",
        "    selected_indices = selector.select_greedy(k=target_size, stratified=True)\n",
        "\n",
        "    # Diversity\n",
        "    diversity_full = selector.compute_diversity_score(list(range(len(texts))))\n",
        "    diversity_selected = selector.compute_diversity_score(selected_indices)\n",
        "\n",
        "    print(f\"\\n Diversity: {diversity_selected/diversity_full*100:.1f}% retained\")\n",
        "\n",
        "    # Save\n",
        "    print(\"\\n[4/4] Saving...\")\n",
        "    selected_data = [(labels[i], texts[i]) for i in selected_indices]\n",
        "\n",
        "    output_file = os.path.join(OUTPUT_DIR, f\"{output_prefix}_selected.txt\")\n",
        "    save_fasttext_data(output_file, selected_data)\n",
        "    print(f\" Saved: {output_file}\")\n",
        "\n",
        "    stats = {\n",
        "        'original_size': len(data),\n",
        "        'selected_size': len(selected_indices),\n",
        "        'positive_count': sum(1 for i in selected_indices if labels[i] == 'positive'),\n",
        "        'negative_count': sum(1 for i in selected_indices if labels[i] == 'negative'),\n",
        "        'diversity_retention': float(diversity_selected / diversity_full)\n",
        "    }\n",
        "\n",
        "    stats_file = os.path.join(OUTPUT_DIR, f\"{output_prefix}_stats.json\")\n",
        "    with open(stats_file, 'w') as f:\n",
        "        json.dump(stats, f, indent=2)\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGDG5SH04HaJ"
      },
      "source": [
        "###Process All Three Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdAGV7x74Nou"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"PROCESSING ALL DATASETS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_stats = {}\n",
        "\n",
        "# Real\n",
        "if os.path.exists(REAL_5K):\n",
        "    all_stats['real'] = process_dataset(REAL_5K, \"real_3k\", target_size=3000)\n",
        "    print(\"\\n Real complete!\\n\")\n",
        "\n",
        "# Synthetic\n",
        "if os.path.exists(SYNTHETIC_5K):\n",
        "    all_stats['synthetic'] = process_dataset(SYNTHETIC_5K, \"synthetic_3k\", target_size=3000)\n",
        "    print(\"\\n Synthetic complete!\\n\")\n",
        "\n",
        "# Mixed\n",
        "if os.path.exists(MIXED_5K):\n",
        "    all_stats['mixed'] = process_dataset(MIXED_5K, \"mixed_3k\", target_size=3000)\n",
        "    print(\"\\n Mixed complete!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3oOzAow4hmo"
      },
      "source": [
        "###Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaFtADxJ4k3_"
      },
      "outputs": [],
      "source": [
        "if all_stats:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(f\"\\n{'Dataset':<15} {'Original':<10} {'Selected':<10} {'Diversity':<12}\")\n",
        "    print(\"-\" * 50)\n",
        "    for name, stats in all_stats.items():\n",
        "        print(f\"{name:<15} {stats['original_size']:<10} \"\n",
        "              f\"{stats['selected_size']:<10} \"\n",
        "              f\"{stats['diversity_retention']*100:.1f}%\")\n",
        "\n",
        "    print(f\"\\nResults in: {OUTPUT_DIR}/\")\n",
        "    print(\"\\nFiles created:\")\n",
        "    for name in all_stats.keys():\n",
        "        print(f\"  • {OUTPUT_DIR}/{name}_3k_selected.txt\")\n",
        "\n",
        "    print(\"\\n Done! \")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCfrZkTIUOVu"
      },
      "source": [
        "# 4. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "G3kMDHGMX_Br"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fasttext\n",
        "import json"
      ],
      "metadata": {
        "id": "noZAtIwDqnbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Hyperparameter Search for FastText (Learning Rate × Epoch)\n",
        "- Used a grid search over multiple learning rates and epoch values to find the best configuration for the FastText classifier trained on the 5k real dataset.\n",
        "- For each combination, we:\n",
        "  1. Train the FastText model\n",
        "  2. Evaluate precision, recall, and F1 score on the validation set\n",
        "  3. Save the trained model\n",
        "  4. Record all results and track the best-performing hyperparameter configuration"
      ],
      "metadata": {
        "id": "0dC1vkFXl4Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lrs = [0.01, 0.05, 0.1, 0.3]\n",
        "epochs = [5, 10, 25, 50]\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "results = []\n",
        "best_score = 0.0\n",
        "best_config = None\n",
        "\n",
        "for lr in lrs:\n",
        "    for epoch in epochs:\n",
        "        print(f\"Training lr={lr}, epoch={epoch}\")\n",
        "        model = fasttext.train_supervised(\n",
        "            input=\"real_5000.train\",\n",
        "            lr=lr,\n",
        "            epoch=epoch,\n",
        "            wordNgrams=2,\n",
        "            dim=100,\n",
        "            loss=\"softmax\",\n",
        "        )\n",
        "\n",
        "        # Validation\n",
        "        # N: # of sample, p: precision, r: recall\n",
        "        N, p, r = model.test(\"real_5000.valid\")\n",
        "\n",
        "        #F1 Score\n",
        "        f1 = 0.0\n",
        "        if p + r > 0:\n",
        "            f1 = 2 * p * r / (p + r)\n",
        "\n",
        "        # Save Model\n",
        "        model_path = f\"models/real_5000_lr{lr}_ep{epoch}.bin\"\n",
        "        model.save_model(model_path)\n",
        "\n",
        "        # Result -> dictionary\n",
        "        result = {\n",
        "            \"lr\": lr,\n",
        "            \"epoch\": epoch,\n",
        "            \"N\": N,\n",
        "            \"precision\": p,\n",
        "            \"recall\": r,\n",
        "            \"f1\": f1,\n",
        "            \"model_path\": model_path,\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "        # Best Score\n",
        "        if f1 > best_score:\n",
        "            best_score = f1\n",
        "            best_config = result\n",
        "\n",
        "with open(\"results_real_5000.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Best config:\", best_config)\n"
      ],
      "metadata": {
        "id": "kyJSM1FSo9FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training\n",
        "- Each dataset (Real, Synthetic, Hybrid) is trained with a FastText classifier using identical settings."
      ],
      "metadata": {
        "id": "9f2uQGcNmi7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before Submodular Data Selection"
      ],
      "metadata": {
        "id": "c_K2jbxwjgeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Real 5000 dataset"
      ],
      "metadata": {
        "id": "xpiLqSCspF2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 4500 real_5k.txt > real_5000.train\n",
        "!tail -n 500 real_5k.txt > real_5000.valid"
      ],
      "metadata": {
        "id": "aoP0MyasjilB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_supervised(input=\"real_5000.train\", lr=0.3, epoch=25, wordNgrams=2,dim=100, loss='softmax')\n",
        "model.save_model(\"real_5000.bin\")\n",
        "model.test(\"real_5000.valid\")"
      ],
      "metadata": {
        "id": "9roESEhgl2c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fb64aa-422f-413c-d57d-480f00be0c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 0.886, 0.886)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Synthetic 5000 dataset"
      ],
      "metadata": {
        "id": "cDrZm894pJMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 4500 shuffledSyntheticData.txt > synthetic_5000.train\n",
        "!tail -n 500 shuffledSyntheticData.txt > synthetic_5000.valid"
      ],
      "metadata": {
        "id": "VrjBVMESd162"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_supervised(input=\"synthetic_5000.train\", lr=0.3, epoch=25, wordNgrams=2,dim=100, loss='softmax')\n",
        "model.save_model(\"synthetic_5000.bin\")\n",
        "model.test(\"synthetic_5000.valid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZUzv5v5d47_",
        "outputId": "d676bcd9-3e0f-455d-ef24-ce64235ffc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 0.996, 0.996)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hybrid 5000 dataset (real+synthetic)"
      ],
      "metadata": {
        "id": "TKB_R2oIplJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 4500 5050RealSynthetic.txt > mixed_5000.train\n",
        "!tail -n 500 5050RealSynthetic.txt > mixed_5000.valid"
      ],
      "metadata": {
        "id": "Lt98YQZ4fPat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.train_supervised(input=\"mixed_5000.train\", lr=0.3, epoch=25, wordNgrams=2,dim=100, loss='softmax')\n",
        "model.save_model(\"mixed_5000.bin\")\n",
        "model.test(\"mixed_5000.valid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-KuNll6fS33",
        "outputId": "d15da710-9368-435d-d133-fffc6dc0dfff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 0.954, 0.954)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hNdupvkuoi"
      },
      "source": [
        "After Submodular Data Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Real 3000 dataset"
      ],
      "metadata": {
        "id": "V0I_BWGBppgF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i11wjNXAkuV-"
      },
      "outputs": [],
      "source": [
        "!head -n 2700 real_3k_selected_shuffled.txt > real_selected.train\n",
        "!tail -n 300 real_3k_selected_shuffled.txt > real_selected.valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCPQ1Lj1k0Fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3199c651-4f0a-43cf-b498-1e798296152c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 0.83, 0.83)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model = fasttext.train_supervised(input=\"real_selected.train\", lr=0.3, epoch=25, wordNgrams=2,dim=100, loss='softmax')\n",
        "model.save_model(\"real_selected.bin\")\n",
        "model.test(\"real_selected.valid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Synthetic 3000 dataset"
      ],
      "metadata": {
        "id": "9z9VfQt9prSP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvklZNLseZpT"
      },
      "outputs": [],
      "source": [
        "!head -n 2700 mixed_3k_selected_shuffled.txt > mixed.train\n",
        "!tail -n 300 mixed_3k_selected_shuffled.txt > mixed.valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP9pSrJNgjo4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e01f9659-0bd0-442f-d554-a87a6915e215"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 0.88, 0.88)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model = fasttext.train_supervised(input=\"mixed.train\", lr=0.3, epoch=25, wordNgrams=2,dim=100, loss='softmax')\n",
        "model.save_model(\"mixed.bin\")\n",
        "model.test(\"mixed.valid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hybrid 3000 dataset"
      ],
      "metadata": {
        "id": "JpH39mRTpu3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMi6N4RAh0pQ"
      },
      "outputs": [],
      "source": [
        "!head -n 2700 synthetic_3k_selected_shuffled.txt > synthetic.train\n",
        "!tail -n 300 synthetic_3k_selected_shuffled.txt > synthetic.valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gc_dl9fhrhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2127e7-4848-411f-ac86-f7223dac5e29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 0.9933333333333333, 0.9933333333333333)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model = fasttext.train_supervised(input=\"synthetic.train\", lr=0.3, epoch=25, wordNgrams=2,dim=100, loss='softmax')\n",
        "model.save_model(\"synthetic.bin\")\n",
        "model.test(\"synthetic.valid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrar2wDsUVI2"
      },
      "source": [
        "# 5. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create 500-sample test sets from IMDB and Amazon datasets"
      ],
      "metadata": {
        "id": "oCouU-f-l-Xn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7o4lho1OYTcH"
      },
      "outputs": [],
      "source": [
        "!head -n 500 imdb_test_500.txt > imdb.test\n",
        "!head -n 500 amazon_test_500.txt > amazon.test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mix the IMDB and Amazon test sets and shuffle them to create a combined test set"
      ],
      "metadata": {
        "id": "FiF46fgZma25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat imdb.test amazon.test | shuf > imdb_amazon.test"
      ],
      "metadata": {
        "id": "aT3Kz2C2mBRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Evaluation    \n",
        "  - Load each trained model    \n",
        "  - Evaluate on:\n",
        "      - imdb.test — In-domain performance\n",
        "      - amazon.test — Out-of-distribution performance\n"
      ],
      "metadata": {
        "id": "c2UTbFoimdgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Before Data Selection\n",
        "1. Real 5000 Dataset"
      ],
      "metadata": {
        "id": "TAhOiRAuoIaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_5k_model = fasttext.load_model(\"real_5000.bin\")\n",
        "real_5k_model.test(\"imdb_amazon.test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPm_Xaob0YY6",
        "outputId": "6ec48dd5-5357-403b-842d-de78ba9908c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 0.822, 0.822)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(real_5k_model.test(\"imdb.test\"))\n",
        "print(real_5k_model.test(\"amazon.test\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5d_ACVoZpGH",
        "outputId": "feb28830-af19-44bc-8f13-178d4c158d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 0.88, 0.88)\n",
            "(500, 0.764, 0.764)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Synthetic 5000 Dataset"
      ],
      "metadata": {
        "id": "cxL-Obr1ohGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.load_model(\"synthetic_5000.bin\")\n",
        "model.test(\"imdb_amazon.test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku1YKViCg_de",
        "outputId": "63b1c551-4bbe-4b44-a3f3-312b75410271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 0.614, 0.614)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.test(\"imdb.test\"))\n",
        "print(model.test(\"amazon.test\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SehTS2bUZwuP",
        "outputId": "3421f24e-1c63-4b89-8225-0fc5e9d9bc30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 0.63, 0.63)\n",
            "(500, 0.598, 0.598)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Hybrid 5000 Dataset (Real + Synthetic)"
      ],
      "metadata": {
        "id": "mIArgBv_oi94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCQTXxqhksLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646e014a-28e3-480c-f313-b291e3f703df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 0.687, 0.687)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "model = fasttext.load_model(\"mixed_5000.bin\")\n",
        "model.test(\"imdb_amazon.test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.test(\"imdb.test\"))\n",
        "print(model.test(\"amazon.test\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8cme7bPZ2I_",
        "outputId": "87f2b8b6-5f34-4da8-8cb3-8a76550ea276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 0.688, 0.688)\n",
            "(500, 0.686, 0.686)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### After Data Selection\n",
        "1. Real 3000 Dataset"
      ],
      "metadata": {
        "id": "CgcOuShnoNe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = fasttext.load_model(\"real_selected.bin\")\n",
        "model.test(\"imdb_amazon.test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1q5A8ptlkdy",
        "outputId": "c2977793-c4f7-4a48-9b4b-98522f2d58ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 0.819, 0.819)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.test(\"imdb.test\"))\n",
        "print(model.test(\"amazon.test\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN4Zg45mZ42K",
        "outputId": "8e4978b2-91a4-4cba-c85a-9ac2a7394597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 0.848, 0.848)\n",
            "(500, 0.79, 0.79)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Synthetic 3000 Dataset"
      ],
      "metadata": {
        "id": "23zGFPfposhQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYnDahEZknOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92ae353-4b3c-46d8-f4aa-cd5cb41ba085"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 0.608, 0.608)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "model = fasttext.load_model(\"synthetic.bin\")\n",
        "model.test(\"imdb_amazon.test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.test(\"imdb.test\"))\n",
        "print(model.test(\"amazon.test\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lX_AoIxZ6iZ",
        "outputId": "6a1af9c6-c6be-4d2d-e8a6-1b42d6b61164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 0.628, 0.628)\n",
            "(500, 0.588, 0.588)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Hybrid 3000 Dataset (Real + Synthetic)"
      ],
      "metadata": {
        "id": "l3O4qvgeouzo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxKOnCVGkkv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3fbb8d-f9f8-4c31-f286-1d1b687421d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 0.815, 0.815)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model = fasttext.load_model(\"mixed.bin\")\n",
        "model.test(\"imdb_amazon.test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.test(\"imdb.test\"))\n",
        "print(model.test(\"amazon.test\"))"
      ],
      "metadata": {
        "id": "_GSZoZGAZ8fP",
        "outputId": "932f8918-7de8-4f17-d126-e4c85ba70a49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 0.85, 0.85)\n",
            "(500, 0.78, 0.78)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l_oy0XtsUFFx"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
